{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from transformers import BertTokenizer, BertModel, AdamW, BertForSequenceClassification\n",
    "\n",
    "if torch.cuda.is_available():      \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from transformers import AutoTokenizer, AutoImageProcessor\n",
    "import re\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torchvision.models as models\n",
    "from transformers import BertModel\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # process image and text seperately\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.PRED_LABEL = [\n",
    "            'Atelectasis',\n",
    "            'Cardiomegaly', \n",
    "            'Consolidation',\n",
    "            'Edema',\n",
    "            'Enlarged Cardiomediastinum',\n",
    "            'Fracture',\n",
    "            'Lung Lesion',\n",
    "            'Lung Opacity',\n",
    "            'No Finding',\n",
    "            'Pleural Effusion',\n",
    "            'Pleural Other',\n",
    "            'Pneumonia',\n",
    "            'Pneumothorax',\n",
    "            'Support Devices']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        # Load text\n",
    "        text_path = self.df.iloc[idx]['text_path']\n",
    "        text_path = \"/scratch/tg2426/MIMIC_CLIP/\" + text_path\n",
    "        with open(text_path, 'r') as f:\n",
    "            text = f.read()\n",
    "        # clean special character, \\n and extra space\n",
    "        clean_text = re.sub('[\\\\(\\[#.!?,\\'\\/\\])0-9]', ' ', str(text))\n",
    "        clean_text = clean_text.replace('\\n', ' ').replace('\\r', '')\n",
    "        clean_text = ' '.join(clean_text.split())\n",
    "        encoding = self.tokenizer(\n",
    "            clean_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Load label\n",
    "        label = torch.FloatTensor(np.zeros(len(self.PRED_LABEL), dtype=float))\n",
    "        for i in range(0, len(self.PRED_LABEL)):\n",
    "            if (self.df[self.PRED_LABEL[i].strip()].iloc[idx].astype('float') > 0):\n",
    "                label[i] = self.df[self.PRED_LABEL[i].strip()].iloc[idx].astype('float')\n",
    "        \n",
    "        #Prepare inputs for CLIP model\n",
    "        \n",
    "        return (encoding['input_ids'].squeeze(0).clone().detach() , encoding['attention_mask'].squeeze(0).clone().detach(), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## load csv file, and dataloader######################\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(MIMICDataset('/'), batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(MIMICDataset(''), batch_size=BATCH_SIZE, shuffle=True) \n",
    "test_loader = DataLoader(MIMICDataset(''), batch_size=BATCH_SIZE, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, num_epochs=5):\n",
    "    train_loss_list, train_acc_list, valid_loss_list, valid_acc_list = [], [], [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = 0, 0\n",
    "        model.train()\n",
    "        for (input_ids, attention_mask, labels) in train_loader:\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            output = output['logits']\n",
    "#             print(output)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * input_ids.size(0)\n",
    "            predicted = torch.round(output)\n",
    "            train_acc += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc /= (len(train_loader.dataset)*14)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        \n",
    "        #### valid ####\n",
    "        val_loss, val_acc = 0, 0\n",
    "        model.eval()\n",
    "        for (input_ids, attention_mask,labels) in valid_loader:\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            output = output['logits']\n",
    "            predicted = torch.round(output)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.item()*input_ids.size(0)\n",
    "            val_acc += (predicted == labels).sum().item()\n",
    "      \n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "        val_acc /= (len(valid_loader.dataset)*14)\n",
    "        valid_loss_list.append(val_loss)\n",
    "        valid_acc_list.append(val_acc)    \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTrain Accuracy: {:.6f} \\tValidation Accuracy: {:.6f} '.format(\n",
    "            epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "#         torch.save(model.state_dict(), \"./clip_test_result/model/\" + 'clip_classify{}.pt'.format(epoch + 1))\n",
    "    return train_loss_list, train_acc_list, valid_loss_list, valid_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 4.095170 \tValidation Loss: 4.110264 \tTrain Accuracy: 0.285153 \tValidation Accuracy: 0.283143 \n",
      "Epoch: 1 \tTraining Loss: 4.134522 \tValidation Loss: 3.992084 \tTrain Accuracy: 0.283867 \tValidation Accuracy: 0.247048 \n",
      "Epoch: 2 \tTraining Loss: 4.094620 \tValidation Loss: 4.005354 \tTrain Accuracy: 0.284316 \tValidation Accuracy: 0.413571 \n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=14).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_loss_list, train_acc_list, valid_loss_list, valid_acc_list = train(model, optimizer, criterion, train_loader, valid_loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
